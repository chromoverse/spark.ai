{
  "models": {
    "qwen2.5-coder-1.5b": {
      "cpu": {
        "url": "https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF/resolve/main/qwen2.5-coder-1.5b-instruct-q4_k_m.gguf",
        "filename": "qwen2.5-coder-1.5b-instruct-q4_k_m.gguf",
        "size_gb": 1.0,
        "quantization": "q4_k_m",
        "description": "Best for JSON / schema tasks"
      },
      "gpu": {
        "url": "https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF/resolve/main/qwen2.5-coder-1.5b-instruct-q8_0.gguf",
        "filename": "qwen2.5-coder-1.5b-instruct-q8_0.gguf",
        "size_gb": 1.6,
        "quantization": "q8_0",
        "description": "High precision GPU"
      },
      "vulkan": {
        "url": "https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF/resolve/main/qwen2.5-coder-1.5b-instruct-q4_k_m.gguf",
        "filename": "qwen2.5-coder-1.5b-instruct-q4_k_m.gguf",
        "size_gb": 1.0,
        "quantization": "q4_k_m",
        "description": "Vulkan compatible"
      }
    },

    "smollm2-1.7b": {
      "cpu": {
        "url": "https://huggingface.co/bartowski/SmolLM2-1.7B-Instruct-GGUF/resolve/main/SmolLM2-1.7B-Instruct-Q4_K_M.gguf",
        "filename": "SmolLM2-1.7B-Instruct-Q4_K_M.gguf",
        "size_gb": 1.1,
        "quantization": "Q4_K_M",
        "description": "Small instruct model"
      },
      "gpu": {
        "url": "https://huggingface.co/bartowski/SmolLM2-1.7B-Instruct-GGUF/resolve/main/SmolLM2-1.7B-Instruct-Q8_0.gguf",
        "filename": "SmolLM2-1.7B-Instruct-Q8_0.gguf",
        "size_gb": 1.8,
        "quantization": "Q8_0",
        "description": "Small GPU"
      },
      "vulkan": {
        "url": "https://huggingface.co/bartowski/SmolLM2-1.7B-Instruct-GGUF/resolve/main/SmolLM2-1.7B-Instruct-Q4_K_M.gguf",
        "filename": "SmolLM2-1.7B-Instruct-Q4_K_M.gguf",
        "size_gb": 1.1,
        "quantization": "Q4_K_M",
        "description": "Vulkan compatible"
      }
    },

    "llama-3.2-1b": {
      "cpu": {
        "url": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf",
        "filename": "Llama-3.2-1B-Instruct-Q4_K_M.gguf",
        "size_gb": 0.75,
        "quantization": "Q4_K_M",
        "description": "Smallest Llama"
      },
      "gpu": {
        "url": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q8_0.gguf",
        "filename": "Llama-3.2-1B-Instruct-Q8_0.gguf",
        "size_gb": 1.3,
        "quantization": "Q8_0",
        "description": "Small Llama GPU"
      },
      "vulkan": {
        "url": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf",
        "filename": "Llama-3.2-1B-Instruct-Q4_K_M.gguf",
        "size_gb": 0.75,
        "quantization": "Q4_K_M",
        "description": "Small Vulkan Llama"
      }
    },

    "qwen2.5-7b": {
      "cpu": {
        "url": "https://huggingface.co/wanhin/qwen2.5-7b-instruct-gguf/resolve/main/qwen2.5-7b-instruct-q4_k_m.gguf",
        "filename": "qwen2.5-7b-instruct-q4_k_m.gguf",
        "size_gb": 4.68,
        "quantization": "Q4_K_M",
        "description": "Primary 7B — CPU/Vulkan baseline"
      },
      "gpu": {
        "url": "https://huggingface.co/wanhin/qwen2.5-7b-instruct-gguf/resolve/main/qwen2.5-7b-instruct-q6_k.gguf",
        "filename": "qwen2.5-7b-instruct-q6_k.gguf",
        "size_gb": 5.82,
        "quantization": "Q6_K",
        "description": "Primary 7B — GPU"
      },
      "vulkan": {
        "url": "https://huggingface.co/wanhin/qwen2.5-7b-instruct-gguf/resolve/main/qwen2.5-7b-instruct-q4_k_m.gguf",
        "filename": "qwen2.5-7b-instruct-q4_k_m.gguf",
        "size_gb": 4.68,
        "quantization": "Q4_K_M",
        "description": "Primary 7B Vulkan"
      }
    },

    "mistral-7b": {
      "cpu": {
        "url": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
        "filename": "mistral-7b-instruct-v0.2-q4_k_m.gguf",
        "size_gb": 4.37,
        "quantization": "Q4_K_M",
        "description": "Fast 7B — CPU/Vulkan safe"
      },
      "gpu": {
        "url": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q6_K.gguf",
        "filename": "mistral-7b-instruct-v0.2-q6_k.gguf",
        "size_gb": 5.94,
        "quantization": "Q6_K",
        "description": "Fast 7B — GPU"
      },
      "vulkan": {
        "url": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
        "filename": "mistral-7b-instruct-v0.2-q4_k_m.gguf",
        "size_gb": 4.37,
        "quantization": "Q4_K_M",
        "description": "Fast 7B Vulkan"
      }
    },

    "qwen2.5-1.5b": {
      "cpu": {
        "url": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_k_m.gguf",
        "filename": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
        "size_gb": 0.98,
        "quantization": "Q4_K_M",
        "description": "Tiny CPU/Vulkan"
      },
      "gpu": {
        "url": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q8_0.gguf",
        "filename": "qwen2.5-1.5b-instruct-q8_0.gguf",
        "size_gb": 1.6,
        "quantization": "Q8_0",
        "description": "Tiny GPU"
      },
      "vulkan": {
        "url": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_k_m.gguf",
        "filename": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
        "size_gb": 0.98,
        "quantization": "Q4_K_M",
        "description": "Tiny Vulkan"
      }
    }
  },
  "llama_cpp_binaries": {
    "cpu": {
      "url": "https://github.com/ggerganov/llama.cpp/releases/download/b4480/llama-b4480-bin-win-avx2-x64.zip",
      "filename": "llama-b4480-bin-win-avx2-x64.zip",
      "executable": "llama-cli.exe",
      "size_mb": 50,
      "description": "CPU-optimized (AVX2)"
    },
    "gpu": {
      "url": "https://github.com/ggerganov/llama.cpp/releases/download/b4480/llama-b4480-bin-win-cuda-cu12.2.0-x64.zip",
      "filename": "llama-b4480-bin-win-cuda-cu12.2.0-x64.zip",
      "executable": "llama-cli.exe",
      "size_mb": 450,
      "description": "GPU-optimized (CUDA 12.2)"
    }
  },
  "settings": {
    "models_base_dir": "qwen",
    "binaries_base_dir": "tools",
    "auto_detect_device": true,
    "force_cpu": false
  }
}
