# test_real_world_scenarios.py


import asyncio
import logging
import json
from datetime import datetime

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

from app.registry.loader import load_tool_registry
from app.core.orchestrator import init_orchestrator
from app.core.execution_engine import init_execution_engine
from app.core.server_executor import init_server_executor
from app.core.task_emitter import get_task_emitter
from app.core.models import Task, LifecycleMessages
from app.tools.loader import load_all_tools


def print_section(title: str):
    """Pretty section divider"""
    logger.info("\n" + "="*80)
    logger.info(f"  {title}")
    logger.info("="*80 + "\n")


async def scenario_1_server_chain():
    """
    Scenario 1: Sâ†’S (Server Chain)
    
    search_company â†’ fetch_details
    
    Tests:
    - Server task dependency
    - Input binding from server to server
    - Sequential server execution
    """
    print_section("SCENARIO 1: Server Chain (Sâ†’S)")
    
    orchestrator = init_orchestrator()
    server_executor = init_server_executor()
    execution_engine = init_execution_engine()
    execution_engine.set_server_executor(server_executor)
    # âœ… Add mock client emitter
    mock_emitter = get_task_emitter()
    execution_engine.set_client_emitter(mock_emitter)
    
    user_id = "scenario_1"
    
    tasks = [
        Task(
            task_id="search_company",
            tool="web_search",
            execution_target="server",
            depends_on=[],
            inputs={"query": "Anthropic AI company"},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ” Searching for company info...",
                on_success="âœ… Found company information",
                on_failure="âŒ Search failed"
            )
        ),
        Task(
            task_id="fetch_details",
            tool="web_search",
            execution_target="server",
            depends_on=["search_company"],
            inputs={"query": "Anthropic Claude API pricing"},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ“¡ Fetching additional details...",
                on_success="âœ… Details retrieved",
                on_failure="âŒ Fetch failed"
            )
        ),
    ]
    
    await orchestrator.register_tasks(user_id, tasks)
    engine_task = await execution_engine.start_execution(user_id)
    await engine_task
    
    summary = await orchestrator.get_execution_summary(user_id)
    logger.info(f"\nğŸ“Š Summary: {summary}")
    
    return summary


async def scenario_2_parallel_servers():
    """
    Scenario 2: S1, S2 (Independent Parallel Server Tasks)
    
    search_weather + search_stocks (parallel)
    
    Tests:
    - Parallel server execution
    - No dependencies
    - Concurrent tool execution
    """
    print_section("SCENARIO 2: Parallel Independent Servers (S1, S2)")
    
    orchestrator = init_orchestrator()
    server_executor = init_server_executor()
    execution_engine = init_execution_engine()
    execution_engine.set_server_executor(server_executor)
    
    user_id = "scenario_2"
    
    tasks = [
        Task(
            task_id="search_weather",
            tool="web_search",
            execution_target="server",
            depends_on=[],
            inputs={"query": "Tokyo weather today"},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸŒ¤ï¸  Checking weather...",
                on_success="âœ… Weather data retrieved"
            )
        ),
        Task(
            task_id="search_stocks",
            tool="web_search",
            execution_target="server",
            depends_on=[],
            inputs={"query": "NASDAQ today"},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ“ˆ Fetching stock data...",
                on_success="âœ… Stock data retrieved"
            )
        ),
        Task(
            task_id="search_crypto",
            tool="web_search",
            execution_target="server",
            depends_on=[],
            inputs={"query": "Bitcoin price"},
            lifecycle_messages=LifecycleMessages(
                on_start="â‚¿ Checking crypto prices...",
                on_success="âœ… Crypto data retrieved"
            )
        ),
    ]
    
    await orchestrator.register_tasks(user_id, tasks)
    
    start_time = datetime.now()
    engine_task = await execution_engine.start_execution(user_id)
    await engine_task
    duration = (datetime.now() - start_time).total_seconds()
    
    summary = await orchestrator.get_execution_summary(user_id)
    logger.info(f"\nğŸ“Š Summary: {summary}")
    logger.info(f"â±ï¸  Total execution time: {duration:.2f}s (should be ~same as single task due to parallelism)")
    
    return summary

async def scenario_3_server_to_client_chain():
    """
    Scenario 3: Sâ†’Câ†’C (Server triggers Client Chain)
    
    fetch_data â†’ open_notepad
    
    Tests:
    - Server to client handoff
    - Client task execution after server completion
    - Input binding across execution targets
    """
    print_section("SCENARIO 3: Server to Client Chain (Sâ†’C)")
    
    orchestrator = init_orchestrator()
    server_executor = init_server_executor()
    execution_engine = init_execution_engine()
    execution_engine.set_server_executor(server_executor)
    
    # âœ… Add mock client emitter
    mock_emitter = get_task_emitter()
    execution_engine.set_client_emitter(mock_emitter)
    
    user_id = "scenario_3"
    
    tasks = [
        Task(
            task_id="fetch_data",
            tool="web_search",
            execution_target="server",
            depends_on=[],
            inputs={"query": "latest AI research papers"},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ“¥ Fetc`hing research data from web...",
                on_success="âœ… Data fetched successfully"
            )
        ),
        Task(
            task_id="open_notepad",
            tool="open_app",
            execution_target="client",
            depends_on=[],
            inputs={"target": "notepad"},
            input_bindings={},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ“ Opening Notepad on client...",
                on_success="âœ… Notepad opened successfully"
            )
        ),
        Task(
            task_id="open_vscode",
            tool="open_app",
            execution_target="client",
            depends_on=[],
            inputs={"target": "vscode"},
            input_bindings={},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ“ Opening VSCode on client...",
                on_success="âœ… VSCode opened successfully"
            )
        ),
        Task(
            task_id="open_zen",
            tool="open_app",
            execution_target="client",
            depends_on=[],
            inputs={"target": "Zen"},
            input_bindings={},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ“ Opening Notepad on client...",
                on_success="âœ… Notepad opened successfully"
            )
        )
    ]
    
    await orchestrator.register_tasks(user_id, tasks)
    engine_task = await execution_engine.start_execution(user_id)
    await engine_task
    
    summary = await orchestrator.get_execution_summary(user_id)
    logger.info(f"\nğŸ“Š Summary: {summary}")
    
    # Verify serverâ†’client handoff
    state = orchestrator.get_state(user_id)
    if state:
        fetch_task = state.get_task("fetch_data")
        notepad_task = state.get_task("open_notepad")
        
        if fetch_task and notepad_task:
            logger.info(f"\nğŸ”— Serverâ†’Client handoff verification:")
            logger.info(f"   fetch_data completed at: {fetch_task.completed_at}")
            logger.info(f"   open_notepad emitted at: {notepad_task.emitted_at}")
            logger.info(f"   âœ… Client task emitted after server completion!")
        else:
            logger.warning(f"âš ï¸ Tasks not found - fetch_task: {fetch_task}, notepad_task: {notepad_task}")
    
    return summary


async def scenario_4_pure_client_chain():
    """
    Scenario 4: Câ†’Câ†’C (Pure Client Chain)
    
    create_project â†’ create_files â†’ update_config
    
    Tests:
    - Client-only workflow
    - Multi-step client chain
    - Batch emission optimization
    - Local dependency handling
    """
    print_section("SCENARIO 4: Pure Client Chain (Câ†’Câ†’C)")
    
    orchestrator = init_orchestrator()
    execution_engine = init_execution_engine()
    # âœ… Add mock client emitter  
    mock_emitter = get_task_emitter()
    execution_engine.set_client_emitter(mock_emitter)
    # âœ… Add mock client emitter
    mock_emitter = get_task_emitter()
    execution_engine.set_client_emitter(mock_emitter)
    
    user_id = "scenario_4"
    
    tasks = [
        Task(
            task_id="create_project",
            tool="file_open",
            execution_target="client",
            depends_on=[],
            inputs={"path": "~/my_project"},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ“ Creating project folder...",
                on_success="âœ… Project folder created"
            )
        ),
        Task(
            task_id="create_files",
            tool="file_create",
            execution_target="client",
            depends_on=["create_project"],
            inputs={"path": "~/my_project/main.py", "content": "# Project code"},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ“„ Creating project files...",
                on_success="âœ… Files created"
            )
        ),
        Task(
            task_id="update_config",
            tool="file_create",
            execution_target="client",
            depends_on=["create_files"],
            inputs={"path": "~/my_project/config.json", "content": '{"version": "1.0"}'},
            lifecycle_messages=LifecycleMessages(
                on_start="âš™ï¸  Updating configuration...",
                on_success="âœ… Config updated"
            )
        ),
    ]
    
    await orchestrator.register_tasks(user_id, tasks)
    engine_task = await execution_engine.start_execution(user_id)
    await engine_task
    
    summary = await orchestrator.get_execution_summary(user_id)
    logger.info(f"\nğŸ“Š Summary: {summary}")
    
    logger.info("\nğŸš€ This entire chain should be emitted as ONE batch to client!")
    
    return summary


async def scenario_5_parallel_clients():
    """
    Scenario 5: C1, C2 (Independent Parallel Client Tasks)
    
    create_notes + create_images (parallel)
    
    Tests:
    - Parallel client execution
    - Independent client tasks
    - No cross-dependencies
    """
    print_section("SCENARIO 5: Parallel Independent Clients (C1, C2)")
    
    orchestrator = init_orchestrator()
    execution_engine = init_execution_engine()
    
    user_id = "scenario_5"
    
    tasks = [
        Task(
            task_id="create_notes",
            tool="file_create",
            execution_target="client",
            depends_on=[],
            inputs={"path": "~/notes.txt", "content": "Meeting notes"},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ“ Creating notes file...",
                on_success="âœ… Notes created"
            )
        ),
        Task(
            task_id="create_images",
            tool="folder_create",
            execution_target="client",
            depends_on=[],
            inputs={"path": "~/images"},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ–¼ï¸  Creating images folder...",
                on_success="âœ… Images folder created"
            )
        ),
        Task(
            task_id="create_backup",
            tool="folder_create",
            execution_target="client",
            depends_on=[],
            inputs={"path": "~/backup"},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ’¾ Creating backup folder...",
                on_success="âœ… Backup folder created"
            )
        ),
    ]
    
    await orchestrator.register_tasks(user_id, tasks)
    engine_task = await execution_engine.start_execution(user_id)
    await engine_task
    
    summary = await orchestrator.get_execution_summary(user_id)
    logger.info(f"\nğŸ“Š Summary: {summary}")
    logger.info(f"\nâš¡ These tasks should be emitted separately for parallel client execution!")
    
    return summary


async def scenario_6_complex_mixed():
    """
    Scenario 6: Complex Mixed Workflow
    
    Graph:
    ```
    search1 (S) â”€â”
                 â”œâ”€> analyze (S) â”€> create_report_folder (C) â”€> write_report (C)
    search2 (S) â”€â”˜                          â”‚
                                            â”œâ”€> write_summary (C)
                                            â”‚
    fetch_images (S) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> save_images (C)
    ```
    
    Tests:
    - Complex dependency graph
    - Mixed server/client
    - Multiple parallel branches
    - Input binding across targets
    - Chain detection in complex graph
    """
    print_section("SCENARIO 6: Complex Mixed Workflow")
    
    orchestrator = init_orchestrator()
    server_executor = init_server_executor()
    execution_engine = init_execution_engine()
    execution_engine.set_server_executor(server_executor)
    
    user_id = "scenario_6"
    
    tasks = [
        # Parallel server searches
        Task(
            task_id="search_market_data",
            tool="web_search",
            execution_target="server",
            depends_on=[],
            inputs={"query": "stock market trends 2024"},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ“Š Searching market data...",
                on_success="âœ… Market data found"
            )
        ),
        Task(
            task_id="search_competitor_data",
            tool="web_search",
            execution_target="server",
            depends_on=[],
            inputs={"query": "competitor analysis tech sector"},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ” Searching competitor data...",
                on_success="âœ… Competitor data found"
            )
        ),
        
        # Server analysis (depends on both searches)
        Task(
            task_id="analyze_combined",
            tool="web_search",
            execution_target="server",
            depends_on=["search_market_data", "search_competitor_data"],
            inputs={"query": "market analysis summary"},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ§  Analyzing combined data...",
                on_success="âœ… Analysis complete"
            )
        ),
        
        # Client chain: create folder â†’ write files
        Task(
            task_id="create_report_folder",
            tool="folder_create",
            execution_target="client",
            depends_on=["analyze_combined"],
            inputs={"path": "~/reports"},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ“ Creating report folder...",
                on_success="âœ… Folder created"
            )
        ),
        Task(
            task_id="write_report",
            tool="file_create",
            execution_target="client",
            depends_on=["create_report_folder"],
            inputs={"path": "~/reports/analysis.txt"},
            input_bindings={
                "content": "$.analyze_combined.data.summary"
            },
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ“ Writing analysis report...",
                on_success="âœ… Report written"
            )
        ),
        Task(
            task_id="write_summary",
            tool="file_create",
            execution_target="client",
            depends_on=["create_report_folder"],
            inputs={"path": "~/reports/summary.txt", "content": "Executive summary"},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ“„ Writing summary...",
                on_success="âœ… Summary written"
            )
        ),
        
        # Parallel independent branch
        Task(
            task_id="fetch_images",
            tool="web_search",
            execution_target="server",
            depends_on=[],
            inputs={"query": "market charts visualization"},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ–¼ï¸  Fetching visualization images...",
                on_success="âœ… Images fetched"
            )
        ),
        Task(
            task_id="save_images",
            tool="folder_create",
            execution_target="client",
            depends_on=["fetch_images"],
            inputs={"path": "~/reports/images"},
            lifecycle_messages=LifecycleMessages(
                on_start="ğŸ’¾ Saving images...",
                on_success="âœ… Images saved"
            )
        ),
    ]
    
    await orchestrator.register_tasks(user_id, tasks)
    
    start_time = datetime.now()
    engine_task = await execution_engine.start_execution(user_id)
    await engine_task
    duration = (datetime.now() - start_time).total_seconds()
    
    summary = await orchestrator.get_execution_summary(user_id)
    logger.info(f"\nğŸ“Š Summary: {summary}")
    logger.info(f"â±ï¸  Total execution time: {duration:.2f}s")
    
    # Verify execution order
    state = orchestrator.get_state(user_id)
    if state:
        logger.info("\nğŸ“‹ Execution Timeline:")
        for task_id in ["search_market_data", "search_competitor_data", "analyze_combined",
                        "create_report_folder", "write_report", "write_summary",
                        "fetch_images", "save_images"]:
            task = state.get_task(task_id)
            if task and task.started_at:
                logger.info(f"   {task_id}: started at {task.started_at.strftime('%H:%M:%S.%f')[:-3]}")
    
    return summary


async def run_all_scenarios():
    """Run all test scenarios"""
    print_section("ğŸš€ REAL-WORLD EXECUTION SCENARIOS TEST SUITE")
    
    results = {}

    
    # Run each scenario
    scenarios = [
        # ("Scenario 1: Sâ†’S", scenario_1_server_chain),
        # ("Scenario 2: S1, S2", scenario_2_parallel_servers),
        ("Scenario 3: Sâ†’Câ†’C", scenario_3_server_to_client_chain),
        # ("Scenario 4: Câ†’Câ†’C", scenario_4_pure_client_chain),
        # ("Scenario 5: C1, C2", scenario_5_parallel_clients),
        # ("Scenario 6: Complex", scenario_6_complex_mixed),
    ]
    
    for name, scenario_func in scenarios:
        try:
            logger.info(f"\nâ–¶ï¸  Running {name}...")
            result = await scenario_func()
            results[name] = {"status": "âœ… PASS", "summary": result}
            logger.info(f"âœ… {name} completed successfully\n")
            await asyncio.sleep(1)  # Brief pause between scenarios
        except Exception as e:
            results[name] = {"status": "âŒ FAIL", "error": str(e)}
            logger.error(f"âŒ {name} failed: {e}\n")
    
    # Final report
    print_section("ğŸ“Š FINAL TEST REPORT")
    
    total = len(scenarios)
    passed = sum(1 for r in results.values() if "PASS" in r["status"])
    failed = total - passed
    
    logger.info(f"Total Scenarios: {total}")
    logger.info(f"âœ… Passed: {passed}")
    logger.info(f"âŒ Failed: {failed}")
    logger.info(f"Success Rate: {(passed/total)*100:.1f}%\n")
    
    # logger.info("Detailed Results:")
    for name, result in results.items():
        logger.info(f"  {result['status']} {name}")
        if "summary" in result:
            s = result["summary"]
            logger.info(f"      Tasks: {s.get('total', 0)}, "
                       f"Completed: {s.get('completed', 0)}, "
                       f"Failed: {s.get('failed', 0)}")
    
    print_section("ğŸ TEST SUITE COMPLETE")


if __name__ == "__main__":
    asyncio.run(run_all_scenarios())